{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re \n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' ironhack s  q website  is        '"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_up(s):\n",
    "    \n",
    "        s=re.sub(r\"http\\S+\", \"\", s)\n",
    "        s=re.sub(r\"\\d{1,}\", \"\", s)\n",
    "        s=re.sub(r'[^a-zA-Z]', \" \", s)\n",
    "    \n",
    "        return s.lower()\n",
    "\n",
    "    \n",
    "clean_up(\"@Ironhack's-#Q website 776-is http://ironhack.com [(2018)]')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ironhack', 's', 'q', 'website', 'is']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(s):\n",
    "    \n",
    "    s=word_tokenize(s)\n",
    "    return s\n",
    "\n",
    "tokenize(\"ironhack s  q website  is\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ironhack', 's', 'q', 'websit', 'is']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def stem_and_lemmatize(l):\n",
    "    ps = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stem=[]\n",
    "    lem=[]\n",
    "    for i in l:\n",
    "        stem.append(ps.stem(i))\n",
    "    for i in stem:\n",
    "        lem.append(lemmatizer.lemmatize(i))\n",
    "\n",
    "    return lem\n",
    "\n",
    "l=['ironhack', 's', 'q', 'website', 'is'] \n",
    "l=stem_and_lemmatize(l)\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ironhack', 'q', 'websit']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "def remove_stopwords(l):\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    wordsFiltered = []   \n",
    "    for word in l:\n",
    "        if word not in stopWords:\n",
    "            wordsFiltered.append(word)\n",
    "\n",
    "    return wordsFiltered\n",
    "\n",
    "\n",
    "remove_stopwords(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "zf = zipfile.ZipFile('Sentiment140.csv.zip') \n",
    "sentiment = pd.read_csv(zf.open('Sentiment140.csv'))\n",
    "# , index_col='CensusId'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentiment=sentiment[0:50000]\n",
    "sentiment = sentiment.sample(n=2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>text processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>381414</th>\n",
       "      <td>0</td>\n",
       "      <td>2052780313</td>\n",
       "      <td>Sat Jun 06 01:59:55 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>sophieholly</td>\n",
       "      <td>@sofiesunshine me too  bless her!</td>\n",
       "      <td>[sofiesunshin, bless]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887490</th>\n",
       "      <td>4</td>\n",
       "      <td>1687011322</td>\n",
       "      <td>Sun May 03 07:30:03 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Kiaroux</td>\n",
       "      <td>Is craving some sushi .. and will tell it to a...</td>\n",
       "      <td>[crave, sushi, tell, anyon, listen, one, give,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1209630</th>\n",
       "      <td>4</td>\n",
       "      <td>1988957701</td>\n",
       "      <td>Sun May 31 23:53:33 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mirandaforwood</td>\n",
       "      <td>(and yes, I know I'm slow to the party here</td>\n",
       "      <td>[ye, know, slow, parti]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198271</th>\n",
       "      <td>0</td>\n",
       "      <td>1971182522</td>\n",
       "      <td>Sat May 30 06:26:48 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ZombieAlice</td>\n",
       "      <td>Working till 6</td>\n",
       "      <td>[work, till]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156528</th>\n",
       "      <td>0</td>\n",
       "      <td>1956136525</td>\n",
       "      <td>Thu May 28 21:15:30 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>EirelavMajere</td>\n",
       "      <td>wishing 4 things I can't have</td>\n",
       "      <td>[wish, thing]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         target          id                          date      flag  \\\n",
       "381414        0  2052780313  Sat Jun 06 01:59:55 PDT 2009  NO_QUERY   \n",
       "887490        4  1687011322  Sun May 03 07:30:03 PDT 2009  NO_QUERY   \n",
       "1209630       4  1988957701  Sun May 31 23:53:33 PDT 2009  NO_QUERY   \n",
       "198271        0  1971182522  Sat May 30 06:26:48 PDT 2009  NO_QUERY   \n",
       "156528        0  1956136525  Thu May 28 21:15:30 PDT 2009  NO_QUERY   \n",
       "\n",
       "                   user                                               text  \\\n",
       "381414      sophieholly                  @sofiesunshine me too  bless her!   \n",
       "887490          Kiaroux  Is craving some sushi .. and will tell it to a...   \n",
       "1209630  mirandaforwood       (and yes, I know I'm slow to the party here    \n",
       "198271      ZombieAlice                                    Working till 6    \n",
       "156528    EirelavMajere                     wishing 4 things I can't have    \n",
       "\n",
       "                                            text processed  \n",
       "381414                               [sofiesunshin, bless]  \n",
       "887490   [crave, sushi, tell, anyon, listen, one, give,...  \n",
       "1209630                            [ye, know, slow, parti]  \n",
       "198271                                        [work, till]  \n",
       "156528                                       [wish, thing]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment['text processed']=sentiment['text'].apply(clean_up)\n",
    "sentiment['text processed']=sentiment['text processed'].apply(tokenize)\n",
    "sentiment['text processed']=sentiment['text processed'].apply(stem_and_lemmatize)\n",
    "sentiment['text processed']=sentiment['text processed'].apply(remove_stopwords)\n",
    "sentiment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining all words \n",
    "all_words=[]\n",
    "for i in sentiment['text processed']:\n",
    "    for word in i:\n",
    "        all_words.append(word)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "freqDist = FreqDist(all_words).most_common(3000)\n",
    "top_3000 = [word[0] for word in freqDist]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "def find_features(document):\n",
    "    words = set(document)\n",
    "    features = {}\n",
    "    for w in top_3000:\n",
    "        features[w] = (w in words)\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    sent=analyzer.polarity_scores(\" \".join(document))\n",
    "    if sent[\"pos\"] > 0.5:\n",
    "        sent = 'Pos'\n",
    "    else:\n",
    "        sent = 'Neg'\n",
    "    return features, sent\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "featuresets= list(sentiment['text processed'].apply(find_features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(featuresets[2000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.862\n",
      "Most Informative Features\n",
      "                    luck = True              Pos : Neg    =     16.7 : 1.0\n",
      "            followfriday = True              Pos : Neg    =     14.0 : 1.0\n",
      "                      ff = True              Pos : Neg    =     13.0 : 1.0\n",
      "                   thanx = True              Pos : Neg    =     13.0 : 1.0\n",
      "                 perfect = True              Pos : Neg    =     13.0 : 1.0\n",
      "                     btw = True              Pos : Neg    =     13.0 : 1.0\n",
      "                    lone = True              Pos : Neg    =     13.0 : 1.0\n",
      "                    safe = True              Pos : Neg    =     13.0 : 1.0\n",
      "                   laugh = True              Pos : Neg    =     13.0 : 1.0\n",
      "              davidarchi = True              Pos : Neg    =     13.0 : 1.0\n",
      "                   black = True              Pos : Neg    =     13.0 : 1.0\n",
      "                   award = True              Pos : Neg    =     10.9 : 1.0\n",
      "                   cheer = True              Pos : Neg    =     10.0 : 1.0\n",
      "                    best = True              Pos : Neg    =      8.6 : 1.0\n",
      "                 handsom = True              Pos : Neg    =      7.8 : 1.0\n"
     ]
    }
   ],
   "source": [
    "train_set, test_set = featuresets[:2000], featuresets[2000:]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    " \t\n",
    "print(nltk.classify.accuracy(classifier, test_set))\n",
    "classifier.show_most_informative_features(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
